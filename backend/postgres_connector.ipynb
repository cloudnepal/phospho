{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-20 21:55:10.322\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.core.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mLoading environment variables from .env file\u001b[0m\n",
      "\u001b[32m2024-11-20 21:55:10.322\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.core.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mENVIRONMENT: test\u001b[0m\n",
      "\u001b[32m2024-11-20 21:55:10.322\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mapp.core.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m75\u001b[0m - \u001b[31m\u001b[1mPHOSPHO_AI_HUB_URL is missing from the environment variables\u001b[0m\n",
      "\u001b[32m2024-11-20 21:55:10.323\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mapp.core.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m165\u001b[0m - \u001b[33m\u001b[1mANYSCALE_API_KEY is missing from the environment variables\u001b[0m\n",
      "\u001b[32m2024-11-20 21:55:10.384\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mapp.core.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m201\u001b[0m - \u001b[33m\u001b[1mTEMPORAL_MTLS_TLS_CERT_BASE64 or TEMPORAL_MTLS_TLS_KEY_BASE64 is missing from the environment variables\u001b[0m\n",
      "\u001b[32m2024-11-20 21:55:10.384\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mapp.core.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m209\u001b[0m - \u001b[33m\u001b[1mAPI_TRIGGER_SECRET is missing from the environment variables\u001b[0m\n",
      "\u001b[32m2024-11-20 21:55:10.384\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mapp.core.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m214\u001b[0m - \u001b[33m\u001b[1mTAK_SEARCH_URL is missing from the environment variables\u001b[0m\n",
      "\u001b[32m2024-11-20 21:55:10.384\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mapp.core.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m217\u001b[0m - \u001b[33m\u001b[1mTAK_APP_API_KEY is missing from the environment variables\u001b[0m\n",
      "\u001b[32m2024-11-20 21:55:12.348\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mapp.services.integrations.argilla\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m33\u001b[0m - \u001b[31m\u001b[1mYour Api endpoint at http://localhost:6900 is not available or not responding: [Errno 61] Connection refused\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe function below is a work in progress to convert a Pydantic model to a SQLModel\\nCould be usefull later\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Literal, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from app.api.platform.models import Pagination\n",
    "from app.db.mongo import get_mongo_db\n",
    "from app.services.mongo.tasks import fetch_flattened_tasks, get_total_nb_of_tasks\n",
    "from app.utils import generate_uuid, slugify_string\n",
    "from fastapi import HTTPException\n",
    "from loguru import logger\n",
    "from pydantic import BaseModel, Field\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import text\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "config = {\n",
    "    \"SQLDB_CONNECTION_STRING\": os.getenv(\"SQLDB_CONNECTION_STRING\"),\n",
    "}\n",
    "\n",
    "\n",
    "class PostgresqlCredentials(BaseModel, extra=\"allow\"):\n",
    "    org_id: str\n",
    "    org_name: str\n",
    "    type: str = \"postgresql\"  # integration type\n",
    "    server: str\n",
    "    database: str\n",
    "    username: str\n",
    "    password: str\n",
    "    # Projects that have started exporting are stored here\n",
    "    projects_started: List[str] = Field(default_factory=list)\n",
    "    # Projects that have finished exporting are stored here\n",
    "    projects_finished: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class PostgresqlIntegration:\n",
    "    \"\"\"\n",
    "    Class to export a project to a dedicated Postgres database\n",
    "    \"\"\"\n",
    "\n",
    "    credentials: Optional[PostgresqlCredentials] = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        org_id: str,\n",
    "        org_name: str,\n",
    "        project_id: Optional[str] = None,\n",
    "        project_name: Optional[str] = None,\n",
    "        org_metadata: Optional[dict] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This class exports a project to a dedicated Postgres database.\n",
    "        \"\"\"\n",
    "        self.org_id = org_id\n",
    "        if self.org_id is None:\n",
    "            raise ValueError(\"No org_id provided\")\n",
    "        self.org_name = org_name\n",
    "        if self.org_name is None:\n",
    "            raise ValueError(\"No org_name provided\")\n",
    "\n",
    "        self.project_id = project_id\n",
    "        self.project_name = project_name\n",
    "        if org_metadata is not None:\n",
    "            # Verify that the org has access to a dedicated Postgres database, based on the metadata\n",
    "            if not org_metadata.get(\"power_bi\", False):\n",
    "                raise HTTPException(\n",
    "                    status_code=400,\n",
    "                    detail=f\"The organization {org_id} doesn't have access to this feature. Please reach out.\",\n",
    "                )\n",
    "\n",
    "        if config.SQLDB_CONNECTION_STRING is None:\n",
    "            logger.error(\"Neon admin credentials are not configured\")\n",
    "            raise HTTPException(\n",
    "                status_code=500,\n",
    "                detail=\"Admin credentials are not configured\",\n",
    "            )\n",
    "\n",
    "    def _connection_string(self) -> str:\n",
    "        # TODO : Add custom connection string in the credentials so that this write operation\n",
    "        # can be done on a different database\n",
    "        if self.credentials is None:\n",
    "            raise ValueError(\"No credentials found\")\n",
    "        return f\"{config.SQLDB_CONNECTION_STRING}/{self.credentials.database}\"\n",
    "\n",
    "    async def load_config(self):\n",
    "        \"\"\"\n",
    "        Load the Postgres credentials from MongoDB.\n",
    "\n",
    "        If the credentials are not found, create them. The database name is the slugified org name. The org name\n",
    "        is unique to each organization.\n",
    "\n",
    "        This drops the database if it already exists and creates a new one.\n",
    "        \"\"\"\n",
    "        if self.credentials is not None:\n",
    "            logger.info(f\"Credentials already loaded for {self.org_id}\")\n",
    "            return self.credentials\n",
    "        mongo_db = await get_mongo_db()\n",
    "        postgres_credentials = await mongo_db[\"integrations\"].find_one(\n",
    "            {\"org_id\": self.org_id},\n",
    "        )\n",
    "        if postgres_credentials is not None:\n",
    "            # Remove the _id field\n",
    "            if \"_id\" in postgres_credentials.keys():\n",
    "                del postgres_credentials[\"_id\"]\n",
    "            if \"type\" not in postgres_credentials.keys():\n",
    "                postgres_credentials[\"type\"] = \"postgresql\"\n",
    "                # update type in MongoDB\n",
    "                await mongo_db[\"integrations\"].update_one(\n",
    "                    {\"org_id\": self.org_id},\n",
    "                    {\"$set\": {\"type\": postgres_credentials[\"type\"]}},\n",
    "                )\n",
    "            if \"org_name\" not in postgres_credentials.keys():\n",
    "                postgres_credentials[\"org_name\"] = self.org_name\n",
    "                # update org_name in MongoDB\n",
    "                await mongo_db[\"integrations\"].update_one(\n",
    "                    {\"org_id\": self.org_id},\n",
    "                    {\"$set\": {\"org_name\": postgres_credentials[\"org_name\"]}},\n",
    "                )\n",
    "\n",
    "            postgres_credentials_valid = PostgresqlCredentials.model_validate(\n",
    "                postgres_credentials\n",
    "            )\n",
    "            self.credentials = postgres_credentials_valid\n",
    "            return\n",
    "\n",
    "        # Create database using the connection to the default database\n",
    "        engine = create_engine(f\"{config.SQLDB_CONNECTION_STRING}/phospho\")\n",
    "        database = slugify_string(self.org_name)\n",
    "        # Create the database if it doesn't exist\n",
    "        with engine.connect() as connection:\n",
    "            logger.debug(f\"Creating database {slugify_string(self.org_name)}\")\n",
    "            connection.execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "            # UPDATE pg_database SET datallowconn = 'false' WHERE datname = 'databasename';\n",
    "            # SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = 'databasename';\n",
    "            try:\n",
    "                connection.execute(\n",
    "                    text(f\"DROP DATABASE IF EXISTS {slugify_string(self.org_name)};\")\n",
    "                )\n",
    "            except Exception as e:\n",
    "                # Carry on\n",
    "                logger.error(e)\n",
    "            try:\n",
    "                connection.execute(\n",
    "                    text(f\"CREATE DATABASE {slugify_string(self.org_name)};\")\n",
    "                )\n",
    "            except Exception as e:\n",
    "                # Carry on\n",
    "                logger.error(e)\n",
    "\n",
    "        # Connect to the new database\n",
    "        engine = create_engine(f\"{config.SQLDB_CONNECTION_STRING}/{database}\")\n",
    "        with engine.connect() as connection:\n",
    "            # Create a new user if it doesn't exist\n",
    "            username = f\"user_{generate_uuid()[:8]}\"\n",
    "            password = generate_uuid()\n",
    "            connection.execute(text(f\"DROP USER IF EXISTS {username};\"))\n",
    "            connection.execute(\n",
    "                text(f\"CREATE USER {username} WITH PASSWORD '{password}';\")\n",
    "            )\n",
    "            # Grant all privileges to the user over the database\n",
    "            connection.execute(\n",
    "                text(\n",
    "                    f\"GRANT ALL PRIVILEGES ON DATABASE {slugify_string(self.org_name)} TO {username};\"\n",
    "                )\n",
    "            )\n",
    "            # Grant the select privilege to the user to current tables\n",
    "            connection.execute(\n",
    "                text(f\"GRANT SELECT ON ALL TABLES IN SCHEMA public TO {username};\")\n",
    "            )\n",
    "            # Grant the select privilege to the user to future tables\n",
    "            connection.execute(\n",
    "                text(\n",
    "                    f\"ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO {username};;\"\n",
    "                )\n",
    "            )\n",
    "            # The context manager will commit and close the connection\n",
    "\n",
    "        # Get the server from the connection string\n",
    "        server = config.SQLDB_CONNECTION_STRING.split(\"@\")[1]\n",
    "        self.credentials = PostgresqlCredentials(\n",
    "            org_id=self.org_id,\n",
    "            org_name=self.org_name,\n",
    "            type=\"postgresql\",\n",
    "            server=server,\n",
    "            database=slugify_string(self.org_name),\n",
    "            username=username,\n",
    "            password=password,\n",
    "        )\n",
    "        # Push to MongoDB\n",
    "        await mongo_db[\"integrations\"].update_one(\n",
    "            {\"org_id\": self.org_id},\n",
    "            {\"$set\": self.credentials.model_dump()},\n",
    "            upsert=True,\n",
    "        )\n",
    "\n",
    "    async def update_status(\n",
    "        self,\n",
    "        status: Literal[\"started\", \"failed\", \"finished\"],\n",
    "    ) -> PostgresqlCredentials:\n",
    "        mongo_db = await get_mongo_db()\n",
    "\n",
    "        # Credentials have two array fields projects_started and projects_finished\n",
    "        # We add the project_id to projects_started when the project is started\n",
    "        # We remove the project_id from projects_started when the project is failed\n",
    "        # We remove the project_id from projects_started and add it to projects_finished when the project is finished\n",
    "        if status == \"started\":\n",
    "            updated_credentials = await mongo_db[\"integrations\"].find_one_and_update(\n",
    "                {\"org_id\": self.org_id},\n",
    "                {\"$addToSet\": {\"projects_started\": self.project_id}},\n",
    "                return_document=True,\n",
    "            )\n",
    "        elif status == \"failed\":\n",
    "            updated_credentials = await mongo_db[\"integrations\"].find_one_and_update(\n",
    "                {\"org_id\": self.org_id},\n",
    "                {\"$pull\": {\"projects_started\": self.project_id}},\n",
    "                return_document=True,\n",
    "            )\n",
    "        elif status == \"finished\":\n",
    "            updated_credentials = await mongo_db[\"integrations\"].find_one_and_update(\n",
    "                {\"org_id\": self.org_id},\n",
    "                {\n",
    "                    \"$pull\": {\"projects_started\": self.project_id},\n",
    "                    \"$addToSet\": {\"projects_finished\": self.project_id},\n",
    "                },\n",
    "                return_document=True,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown status {status}\")\n",
    "\n",
    "        return updated_credentials\n",
    "\n",
    "    async def push(\n",
    "        self,\n",
    "        batch_size: int = 256,\n",
    "    ) -> Literal[\"success\", \"failure\"]:\n",
    "        \"\"\"\n",
    "        Export the project to the dedicated Postgres database.\n",
    "\n",
    "        This replaces the table if it already exists.\n",
    "\n",
    "        The table name is the slugified project name.\n",
    "        \"\"\"\n",
    "        if self.project_id is None:\n",
    "            logger.error(\"No project_id provided\")\n",
    "            return \"failure\"\n",
    "        if self.project_name is None:\n",
    "            logger.error(\"No project_name provided\")\n",
    "            return \"failure\"\n",
    "        await self.update_status(\"started\")\n",
    "        await self.load_config()\n",
    "        if self.credentials is None:\n",
    "            logger.error(\"No credentials found\")\n",
    "            await self.update_status(\"failed\")\n",
    "            return \"failure\"\n",
    "\n",
    "        logger.info(\n",
    "            f\"Starting export of project {self.project_id} to dedicated Postgres {self.credentials.server}:{self.credentials.database}\"\n",
    "        )\n",
    "        # Get the total number of tasks\n",
    "        total_nb_tasks = await get_total_nb_of_tasks(self.project_id)\n",
    "        if total_nb_tasks is None or total_nb_tasks == 0:\n",
    "            logger.error(\"No tasks found in the project\")\n",
    "            await self.update_status(\"finished\")\n",
    "            return \"success\"\n",
    "\n",
    "        # Connect to Neon Postgres database, we add asyncpg for async support\n",
    "        debug = config.ENVIRONMENT == \"test\"\n",
    "        try:\n",
    "            engine = create_engine(self._connection_string(), echo=debug)\n",
    "            with engine.connect() as connection:\n",
    "                logger.debug(\n",
    "                    f\"Connected to Postgres {self.credentials.server}:{self.credentials.database}\"\n",
    "                )\n",
    "                nb_batches = total_nb_tasks // batch_size\n",
    "                columns = None\n",
    "                for i in range(nb_batches + 1):\n",
    "                    logger.debug(\n",
    "                        f\"Exporting batch {i}/{nb_batches} ({batch_size} tasks)\"\n",
    "                    )\n",
    "                    flattened_tasks = await fetch_flattened_tasks(\n",
    "                        project_id=self.project_id,\n",
    "                        limit=batch_size,\n",
    "                        with_events=True,\n",
    "                        with_sessions=True,\n",
    "                        pagination=Pagination(page=i, per_page=batch_size),\n",
    "                        sort_get_most_recent=False,\n",
    "                    )\n",
    "                    # Convert the list of FlattenedTask to a pandas dataframe\n",
    "                    tasks_df = pd.DataFrame(\n",
    "                        [task.model_dump() for task in flattened_tasks]\n",
    "                    )\n",
    "                    # The metadata columns depends on the tasks. To avoid creating a wrong schema, we only create the schema with the first batch\n",
    "                    # Then we only keep the columns that are in the first batch\n",
    "                    if columns is None:\n",
    "                        columns = tasks_df.columns\n",
    "                    else:\n",
    "                        # Only keep the columns that are in the first batch and remove columns that are not in the first batch\n",
    "                        tasks_df = tasks_df[\n",
    "                            list(set(columns).intersection(set(tasks_df.columns)))\n",
    "                        ]\n",
    "                    if_exists_mode: Literal[\"replace\", \"append\"] = (\n",
    "                        \"replace\" if i == 0 else \"append\"\n",
    "                    )\n",
    "                    # Note: to_sql is not async, we could use asyncpg directly: https://github.com/MagicStack/asyncpg\n",
    "                    pd.DataFrame.to_sql(\n",
    "                        tasks_df,\n",
    "                        slugify_string(self.project_name),\n",
    "                        connection,\n",
    "                        if_exists=if_exists_mode,\n",
    "                        index=False,\n",
    "                    )\n",
    "                    logger.debug(\"Batch uploaded to Postgres\")\n",
    "\n",
    "                connection.close()\n",
    "\n",
    "            logger.info(\"Export finished\")\n",
    "            await self.update_status(\"finished\")\n",
    "            return \"success\"\n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "            await self.update_status(\"failed\")\n",
    "            return \"failure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres = PostgresqlIntegration(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phospho-env-min",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
